%-------------------------------------------------------------------------------
%	SECTION TITLE
%-------------------------------------------------------------------------------
\cvsection{学术论文}

%-------------------------------------------------------------------------------
%	CONTENT
%-------------------------------------------------------------------------------
\begin{cventries}

%---------------------------------------------------------
  \cventry
    {Johnu George, Ce Gao, Richard Liu, Hou Gang Liu, Yuan Tang, Ramdoot Pydipaty, Amit Kumar Saha} % Award
    {A Scalable and Cloud-Native Hyperparameter Tuning System} % Event
    {\href{https://arxiv.org/abs/2006.02085}{arxiv}} % Location
    {2020.06} % Date(s)
    {
      % \begin{cvitems} % Description(s)
      %   \item {
      %     In this paper, we introduce Katib: a scalable, cloud-native, and production-ready hyperparameter tuning system that is agnostic of the underlying machine learning framework. Though there are multiple hyperparameter tuning systems available, this is the first one that caters to the needs of both users and administrators of the system. We present the motivation and design of the system and contrast it with existing hyperparameter tuning systems, especially in terms of multi-tenancy, scalability, fault-tolerance, and extensibility. It can be deployed on local machines, or hosted as a service in on-premise data centers, or in private/public clouds. We demonstrate the advantage of our system using experimental results as well as real-world, production use cases. Katib has active contributors from multiple companies and is open-sourced under the Apache 2.0 license.
      %   }
      % \end{cvitems}
    }

  \cventry
    {Ce Gao, Rui Ren and Hongming Cai} % Award
    {GAI: A Centralized Tree-Based Scheduler for Machine Learning Workload in Large Shared Cluster} % Event
    {ICA3PP'18 (CCF-C)} % Location
    {2018.11} % Date(s)
    {
      % \begin{cvitems} % Description(s)
      %   \item {
      %     本文分析了机器学习模型的训练，识别了训练过程中的短板效应:与 CPU 训练相比，GPU 训练需要更高的网络带宽。这一观察启发了 GAI 的设计，GAI 是一个集中式的调度器，用于机器学习工作负载。它依赖于两种技术：1)树型结构。该结构分层存储集群信息，实现多层调度。2)扩展良好的优先级算法。我们全面考虑了模型培训工作的多个优先级，以支持资源退化和抢占。在 Kubernetes、Kubeflow 和 TensorFlow 上实现了 GAI 的原型。它是通过一个模拟器和一个真正的基于云的集群进行评估的。结果表明，在 DL 模型上，调度吞吐量提高了28\%，训练收敛速度提高了21\%
      %   }
      % \end{cvitems}
    }

%---------------------------------------------------------
  \cventry
    {Xinyuan Huang, Amit Saha, Debojyoti Dutta and Ce Gao} % Award
    {Kubebench: A Benchmarking Platform for ML Workloads} % Event
    {IEEE AI4I'18} % Location
    {2018.9} % Date(s)
    {
      % \begin{cvitems} % Description(s)
      %   \item {
      %     Machine Learning (ML) workloads are becoming mainstream in the enterprise but the plethora of choices aroundML toolkits and multi-cloud infrastructure make it difficult to compare their performance and costs. In this paper, we motivate the need for benchmarking ML systems in a consistent way,discuss the requirements of an ML benchmarking platform, and propose a design that satisfies the requirements. We present Kubebench, an example open-source implementation of an ML benchmarking platform based on Kubeflow, itself an open-source project for managing any ML stack on Kubernetes, a widely used container management platform.
      %   }
      % \end{cvitems}
    }

\end{cventries}
